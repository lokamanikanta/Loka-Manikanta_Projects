{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8125855,"sourceType":"datasetVersion","datasetId":4802232}],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-18T05:08:15.283680Z","iopub.execute_input":"2024-04-18T05:08:15.284105Z","iopub.status.idle":"2024-04-18T05:08:15.288258Z","shell.execute_reply.started":"2024-04-18T05:08:15.284075Z","shell.execute_reply":"2024-04-18T05:08:15.287369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/naveen/PRSA_data_2010.1.1-2014.12.31.csv')","metadata":{"execution":{"iopub.status.busy":"2024-04-18T05:08:15.289969Z","iopub.execute_input":"2024-04-18T05:08:15.290281Z","iopub.status.idle":"2024-04-18T05:08:15.376826Z","shell.execute_reply.started":"2024-04-18T05:08:15.290252Z","shell.execute_reply":"2024-04-18T05:08:15.375807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-04-18T05:08:15.378847Z","iopub.execute_input":"2024-04-18T05:08:15.379532Z","iopub.status.idle":"2024-04-18T05:08:15.400872Z","shell.execute_reply.started":"2024-04-18T05:08:15.379490Z","shell.execute_reply":"2024-04-18T05:08:15.399885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2024-04-18T05:08:15.402536Z","iopub.execute_input":"2024-04-18T05:08:15.402910Z","iopub.status.idle":"2024-04-18T05:08:15.457708Z","shell.execute_reply.started":"2024-04-18T05:08:15.402877Z","shell.execute_reply":"2024-04-18T05:08:15.456325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-18T05:08:15.460536Z","iopub.execute_input":"2024-04-18T05:08:15.460936Z","iopub.status.idle":"2024-04-18T05:08:15.476223Z","shell.execute_reply.started":"2024-04-18T05:08:15.460911Z","shell.execute_reply":"2024-04-18T05:08:15.475172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Air Pollution Forecasting\n* In this tutorial, we are going to use the Air Quality dataset.\n* \n* This is a dataset that reports on the weather and the level of pollution each hour for five years at the US embassy in Beijing, China.\n* \n* The data includes the date-time, the pollution called PM2.5 concentration, and the weather information including dew point, temperature, pressure, wind direction, wind speed and the cumulative number of hours of snow and rain. The complete feature list in the raw data is as follows:\n* \n* No: row number\n* year: year of data in this row\n* month: month of data in this row\n* day: day of data in this row\n* hour: hour of data in this row\n* pm2.5: PM2.5 concentration\n* DEWP: Dew Point\n* TEMP: Temperature\n* PRES: Pressure\n* cbwd: Combined wind direction\n* Iws: Cumulated wind speed\n* Is: Cumulated hours of snow\n* Ir: Cumulated hours of rain\n* We can use this data and frame a forecasting problem where, given the weather conditions and pollution for prior hours, we forecast the pollution at the next hour.","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-04-18T05:08:15.477502Z","iopub.execute_input":"2024-04-18T05:08:15.477840Z","iopub.status.idle":"2024-04-18T05:08:15.492139Z","shell.execute_reply.started":"2024-04-18T05:08:15.477786Z","shell.execute_reply":"2024-04-18T05:08:15.491221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=df.drop(['No'],axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T05:08:15.493165Z","iopub.execute_input":"2024-04-18T05:08:15.493445Z","iopub.status.idle":"2024-04-18T05:08:15.501420Z","shell.execute_reply.started":"2024-04-18T05:08:15.493421Z","shell.execute_reply":"2024-04-18T05:08:15.500655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T05:08:15.502420Z","iopub.execute_input":"2024-04-18T05:08:15.502691Z","iopub.status.idle":"2024-04-18T05:08:15.521629Z","shell.execute_reply.started":"2024-04-18T05:08:15.502668Z","shell.execute_reply":"2024-04-18T05:08:15.520746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['pm2.5'] = df['pm2.5'].fillna(0)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T05:08:15.522725Z","iopub.execute_input":"2024-04-18T05:08:15.523045Z","iopub.status.idle":"2024-04-18T05:08:15.530954Z","shell.execute_reply.started":"2024-04-18T05:08:15.523022Z","shell.execute_reply":"2024-04-18T05:08:15.529968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-04-18T05:08:15.532206Z","iopub.execute_input":"2024-04-18T05:08:15.532948Z","iopub.status.idle":"2024-04-18T05:08:15.556866Z","shell.execute_reply.started":"2024-04-18T05:08:15.532924Z","shell.execute_reply":"2024-04-18T05:08:15.555959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datetime import datetime\ndf['datetime'] = pd.to_datetime(df[['year', 'month', 'day', 'hour']])","metadata":{"execution":{"iopub.status.busy":"2024-04-18T05:08:15.561295Z","iopub.execute_input":"2024-04-18T05:08:15.561893Z","iopub.status.idle":"2024-04-18T05:08:15.580193Z","shell.execute_reply.started":"2024-04-18T05:08:15.561870Z","shell.execute_reply":"2024-04-18T05:08:15.579490Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df= df.drop(['year','month','day','hour'],axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-18T05:08:15.581082Z","iopub.execute_input":"2024-04-18T05:08:15.581340Z","iopub.status.idle":"2024-04-18T05:08:15.587160Z","shell.execute_reply.started":"2024-04-18T05:08:15.581318Z","shell.execute_reply":"2024-04-18T05:08:15.586257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df\ndf['datetime'].nunique()","metadata":{"execution":{"iopub.status.busy":"2024-04-18T05:08:15.588280Z","iopub.execute_input":"2024-04-18T05:08:15.590004Z","iopub.status.idle":"2024-04-18T05:08:15.599062Z","shell.execute_reply.started":"2024-04-18T05:08:15.589980Z","shell.execute_reply":"2024-04-18T05:08:15.598004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# columns = df.columns.tolist()\n\n# # Move the last column to the first position\n# columns = [columns[-1]] + columns[:-1]\n\n# # Reorder the DataFrame columns\n# df = df[columns]","metadata":{"execution":{"iopub.status.busy":"2024-04-18T05:08:15.600428Z","iopub.execute_input":"2024-04-18T05:08:15.600919Z","iopub.status.idle":"2024-04-18T05:08:15.604951Z","shell.execute_reply.started":"2024-04-18T05:08:15.600894Z","shell.execute_reply":"2024-04-18T05:08:15.604062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-04-18T05:08:15.606213Z","iopub.execute_input":"2024-04-18T05:08:15.606473Z","iopub.status.idle":"2024-04-18T05:08:15.628922Z","shell.execute_reply.started":"2024-04-18T05:08:15.606451Z","shell.execute_reply":"2024-04-18T05:08:15.627974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set 'Datetime' column as index\ndf.set_index('datetime', inplace=True)\n# manually specify column names\ndf.columns = ['pollution', 'dew', 'temp', 'press', 'wnd_dir', 'wnd_spd', 'snow', 'rain']\n","metadata":{"execution":{"iopub.status.busy":"2024-04-18T05:08:15.629900Z","iopub.execute_input":"2024-04-18T05:08:15.630180Z","iopub.status.idle":"2024-04-18T05:08:15.637686Z","shell.execute_reply.started":"2024-04-18T05:08:15.630148Z","shell.execute_reply":"2024-04-18T05:08:15.636895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-04-18T05:08:15.638870Z","iopub.execute_input":"2024-04-18T05:08:15.639190Z","iopub.status.idle":"2024-04-18T05:08:15.660037Z","shell.execute_reply.started":"2024-04-18T05:08:15.639162Z","shell.execute_reply":"2024-04-18T05:08:15.659122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-04-18T05:08:15.661159Z","iopub.execute_input":"2024-04-18T05:08:15.661422Z","iopub.status.idle":"2024-04-18T05:08:15.678389Z","shell.execute_reply.started":"2024-04-18T05:08:15.661399Z","shell.execute_reply":"2024-04-18T05:08:15.677538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-04-18T05:08:15.679446Z","iopub.execute_input":"2024-04-18T05:08:15.679691Z","iopub.status.idle":"2024-04-18T05:08:15.696888Z","shell.execute_reply.started":"2024-04-18T05:08:15.679670Z","shell.execute_reply":"2024-04-18T05:08:15.696060Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prepare data for lstm\nfrom pandas import read_csv\nfrom pandas import DataFrame\nfrom pandas import concat\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import MinMaxScaler\n\n# convert series to supervised learning\ndef series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n\tn_vars = 1 if type(data) is list else data.shape[1]\n\tdf = DataFrame(data)\n\tcols, names = list(), list()\n\t# input sequence (t-n, ... t-1)\n\tfor i in range(n_in, 0, -1):\n\t\tcols.append(df.shift(i))\n\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n\t# forecast sequence (t, t+1, ... t+n)\n\tfor i in range(0, n_out):\n\t\tcols.append(df.shift(-i))\n\t\tif i == 0:\n\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n\t\telse:\n\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n\t# put it all together\n\tagg = concat(cols, axis=1)\n\tagg.columns = names\n\t# drop rows with NaN values\n\tif dropnan:\n\t\tagg.dropna(inplace=True)\n\treturn agg\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-18T05:08:15.697996Z","iopub.execute_input":"2024-04-18T05:08:15.699013Z","iopub.status.idle":"2024-04-18T05:08:15.709340Z","shell.execute_reply.started":"2024-04-18T05:08:15.698979Z","shell.execute_reply":"2024-04-18T05:08:15.708385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from  sklearn.preprocessing import LabelEncoder\ncat = df.select_dtypes(include='object')\nle= LabelEncoder()","metadata":{"execution":{"iopub.status.busy":"2024-04-18T05:08:15.710721Z","iopub.execute_input":"2024-04-18T05:08:15.711042Z","iopub.status.idle":"2024-04-18T05:08:15.723960Z","shell.execute_reply.started":"2024-04-18T05:08:15.711009Z","shell.execute_reply":"2024-04-18T05:08:15.723042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat = le.fit_transform(cat)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T05:08:15.725170Z","iopub.execute_input":"2024-04-18T05:08:15.725510Z","iopub.status.idle":"2024-04-18T05:08:15.745934Z","shell.execute_reply.started":"2024-04-18T05:08:15.725479Z","shell.execute_reply":"2024-04-18T05:08:15.745020Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['wnd_dir']=cat","metadata":{"execution":{"iopub.status.busy":"2024-04-18T05:08:15.746833Z","iopub.execute_input":"2024-04-18T05:08:15.747083Z","iopub.status.idle":"2024-04-18T05:08:15.755229Z","shell.execute_reply.started":"2024-04-18T05:08:15.747061Z","shell.execute_reply":"2024-04-18T05:08:15.754301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"values = df.values\nvalues = values.astype('float32')\n# normalize features\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaled = scaler.fit_transform(values)\n# frame as supervised learning\nreframed = series_to_supervised(scaled, 1, 1)\n# drop columns we don't want to predict\n#reframed.drop(reframed.columns[[9,10,11,12,13,14,15]], axis=1, inplace=True)\nprint(reframed.head())\n","metadata":{"execution":{"iopub.status.busy":"2024-04-18T05:08:15.756658Z","iopub.execute_input":"2024-04-18T05:08:15.757146Z","iopub.status.idle":"2024-04-18T05:08:15.788026Z","shell.execute_reply.started":"2024-04-18T05:08:15.757117Z","shell.execute_reply":"2024-04-18T05:08:15.785656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reframed['var1(t)'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-04-18T05:08:15.789416Z","iopub.execute_input":"2024-04-18T05:08:15.789892Z","iopub.status.idle":"2024-04-18T05:08:15.800052Z","shell.execute_reply.started":"2024-04-18T05:08:15.789857Z","shell.execute_reply":"2024-04-18T05:08:15.798633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"values","metadata":{"execution":{"iopub.status.busy":"2024-04-18T05:08:15.801345Z","iopub.execute_input":"2024-04-18T05:08:15.801690Z","iopub.status.idle":"2024-04-18T05:08:15.815659Z","shell.execute_reply.started":"2024-04-18T05:08:15.801658Z","shell.execute_reply":"2024-04-18T05:08:15.814687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# values = reframed.values\n# total_rows = values.shape[0]\n# split_index = total_rows // 2 \n# train = values[:split_index,:]\n# test = values[split_index:, :]\n# # split into input and outputs\n# train_X, train_y = train[:, :-1], train[:, -1]\n# test_X, test_y = test[:, :-1], test[:, -1]\n# # reshape input to be 3D [samples, timesteps, features]\n# train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n# test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n# print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T05:08:15.817429Z","iopub.execute_input":"2024-04-18T05:08:15.817768Z","iopub.status.idle":"2024-04-18T05:08:15.828034Z","shell.execute_reply.started":"2024-04-18T05:08:15.817741Z","shell.execute_reply":"2024-04-18T05:08:15.826912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot  as plt","metadata":{"execution":{"iopub.status.busy":"2024-04-18T05:08:15.829356Z","iopub.execute_input":"2024-04-18T05:08:15.830039Z","iopub.status.idle":"2024-04-18T05:08:15.840166Z","shell.execute_reply.started":"2024-04-18T05:08:15.830012Z","shell.execute_reply":"2024-04-18T05:08:15.839402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from math import sqrt\n# from numpy import concatenate\n# from matplotlib import pyplot\n# from pandas import read_csv\n# from pandas import DataFrame\n# from pandas import concat\n# ...\n# # make a prediction\n# yhat = model.predict(test_X)\n# test_X = test_X.reshape((test_X.shape[0], test_X.shape[1]))\n# # invert scaling for forecast\n# inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)\n# inv_yhat = scaler.inverse_transform(inv_yhat)\n# inv_yhat = inv_yhat[:,0]\n# # invert scaling for actual\n# test_y = test_y.reshape((len(test_y), 1))\n# inv_y = concatenate((test_y, test_X[:, 1:]), axis=1)\n# inv_y = scaler.inverse_transform(inv_y)\n# inv_y = inv_y[:,0]\n# # calculate RMSE\n# rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n# print('Test RMSE: %.3f' % rmse)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T05:08:15.845746Z","iopub.execute_input":"2024-04-18T05:08:15.846067Z","iopub.status.idle":"2024-04-18T05:08:15.850307Z","shell.execute_reply.started":"2024-04-18T05:08:15.846045Z","shell.execute_reply":"2024-04-18T05:08:15.849427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from math import sqrt\nfrom numpy import concatenate\nfrom matplotlib import pyplot\nfrom pandas import read_csv\nfrom pandas import DataFrame\nfrom pandas import concat\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\n \n# convert series to supervised learning\ndef series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n\tn_vars = 1 if type(data) is list else data.shape[1]\n\tdf = DataFrame(data)\n\tcols, names = list(), list()\n\t# input sequence (t-n, ... t-1)\n\tfor i in range(n_in, 0, -1):\n\t\tcols.append(df.shift(i))\n\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n\t# forecast sequence (t, t+1, ... t+n)\n\tfor i in range(0, n_out):\n\t\tcols.append(df.shift(-i))\n\t\tif i == 0:\n\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n\t\telse:\n\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n\t# put it all together\n\tagg = concat(cols, axis=1)\n\tagg.columns = names\n\t# drop rows with NaN values\n\tif dropnan:\n\t\tagg.dropna(inplace=True)\n\treturn agg\n \n# load dataset\n\nvalues = df.values\n# integer encode direction\nencoder = LabelEncoder()\nvalues[:,4] = encoder.fit_transform(values[:,4])\n# ensure all data is float\nvalues = values.astype('float32')\n# normalize features\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaled = scaler.fit_transform(values)\n# frame as supervised learning\nreframed = series_to_supervised(scaled, 1, 1)\n# drop columns we don't want to predict\nreframed.drop(reframed.columns[[9,10,11,12,13,14,15]], axis=1, inplace=True)\nprint(reframed.head())\n \n# split into train and test sets\nvalues = reframed.values\ntotal_rows = values.shape[0]\nsplit_index = total_rows // 2 \ntrain = values[:split_index,:]\ntest = values[split_index:, :]\n# split into input and outputs\ntrain_X, train_y = train[:, :-1], train[:, -1]\ntest_X, test_y = test[:, :-1], test[:, -1]\n# reshape input to be 3D [samples, timesteps, features]\ntrain_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\ntest_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\nprint(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n \n# design network\nmodel = Sequential()\nmodel.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\nmodel.add(Dense(1))\nmodel.compile(loss='mae', optimizer='adam')\n# fit network\nhistory = model.fit(train_X, train_y, epochs=20, batch_size=72, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n# plot history\npyplot.plot(history.history['loss'], label='train')\npyplot.plot(history.history['val_loss'], label='test')\npyplot.legend()\npyplot.show()\n \n# make a prediction\nyhat = model.predict(test_X)\ntest_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n# invert scaling for forecast\ninv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)\ninv_yhat = scaler.inverse_transform(inv_yhat)\ninv_yhat = inv_yhat[:,0]\n# invert scaling for actual\ntest_y = test_y.reshape((len(test_y), 1))\ninv_y = concatenate((test_y, test_X[:, 1:]), axis=1)\ninv_y = scaler.inverse_transform(inv_y)\ninv_y = inv_y[:,0]\n# calculate RMSE\nrmse = sqrt(mean_squared_error(inv_y, inv_yhat))\nprint('Test RMSE: %.3f' % rmse)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T05:08:15.851644Z","iopub.execute_input":"2024-04-18T05:08:15.851979Z","iopub.status.idle":"2024-04-18T05:08:44.267696Z","shell.execute_reply.started":"2024-04-18T05:08:15.851951Z","shell.execute_reply":"2024-04-18T05:08:44.266678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_X.shape[1]","metadata":{"execution":{"iopub.status.busy":"2024-04-18T05:08:44.268866Z","iopub.execute_input":"2024-04-18T05:08:44.269172Z","iopub.status.idle":"2024-04-18T05:08:44.274977Z","shell.execute_reply.started":"2024-04-18T05:08:44.269145Z","shell.execute_reply":"2024-04-18T05:08:44.273997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\n# Assuming y_true contains the actual target values and y_pred contains the predicted values\nmae = mean_absolute_error(inv_y, inv_yhat)\nmse = mean_squared_error(inv_y, inv_yhat)\nrmse = mean_squared_error(inv_y, inv_yhat, squared=False)\n\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\naccurcay = r2_score(inv_y, inv_yhat)\naccurcay","metadata":{"execution":{"iopub.status.busy":"2024-04-18T05:08:44.276109Z","iopub.execute_input":"2024-04-18T05:08:44.276384Z","iopub.status.idle":"2024-04-18T05:08:44.291185Z","shell.execute_reply.started":"2024-04-18T05:08:44.276360Z","shell.execute_reply":"2024-04-18T05:08:44.290351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yhat1 = model.predict(train_X)\ntrain_X = train_X.reshape((train_X.shape[0], train_X.shape[2]))\n                          # invert scaling for forecast train\nfrom numpy import concatenate\ninv_yhat1 = concatenate((yhat1, train_X[:, 1:]), axis=1)\ninv_yhat1 = scaler.inverse_transform(inv_yhat1)\ninv_yhat1 = inv_yhat1[:,0]\n                          # invert scaling for actual train\ntrain_y = train_y.reshape((len(train_y), 1))\ninv_y1 = concatenate((train_y, train_X[:, 1:]), axis=1)\ninv_y1 = scaler.inverse_transform(inv_y1)\ninv_y1 = inv_y1[:,0]\n                          \nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\n# Assuming y_true contains the actual target values and y_pred contains the predicted values\nmae = mean_absolute_error(inv_y1, inv_yhat1)\nmse = mean_squared_error(inv_y1, inv_yhat1)\nrmse = mean_squared_error(inv_y1, inv_yhat1, squared=False)\n\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\naccurcay1 = r2_score(inv_y1, inv_yhat1)\naccurcay1","metadata":{"execution":{"iopub.status.busy":"2024-04-18T05:08:44.292599Z","iopub.execute_input":"2024-04-18T05:08:44.293173Z","iopub.status.idle":"2024-04-18T05:08:45.410878Z","shell.execute_reply.started":"2024-04-18T05:08:44.293141Z","shell.execute_reply":"2024-04-18T05:08:45.409957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from math import sqrt\nfrom numpy import concatenate\nfrom matplotlib import pyplot\nfrom pandas import read_csv\nfrom pandas import DataFrame\nfrom pandas import concat\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\n \n# convert series to supervised learning\ndef series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n\tn_vars = 1 if type(data) is list else data.shape[1]\n\tdf = DataFrame(data)\n\tcols, names = list(), list()\n\t# input sequence (t-n, ... t-1)\n\tfor i in range(n_in, 0, -1):\n\t\tcols.append(df.shift(i))\n\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n\t# forecast sequence (t, t+1, ... t+n)\n\tfor i in range(0, n_out):\n\t\tcols.append(df.shift(-i))\n\t\tif i == 0:\n\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n\t\telse:\n\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n\t# put it all together\n\tagg = concat(cols, axis=1)\n\tagg.columns = names\n\t# drop rows with NaN values\n\tif dropnan:\n\t\tagg.dropna(inplace=True)\n\treturn agg\n \n# load dataset\n\nvalues = df.values\n# integer encode direction\nencoder = LabelEncoder()\nvalues[:,4] = encoder.fit_transform(values[:,4])\n# ensure all data is float\nvalues = values.astype('float32')\n# normalize features\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaled = scaler.fit_transform(values)\n# frame as supervised learning\nreframed = series_to_supervised(scaled, 2, 1)\n# drop columns we don't want to predict\nreframed.drop(reframed.columns[[17,18,19,20,21,22,23]], axis=1, inplace=True)\nprint(reframed.head())\n \n# split into train and test sets\nvalues = reframed.values\nvalues = reframed.values\ntotal_rows = values.shape[0]\nsplit_index = total_rows // 2 \ntrain = values[:split_index,:]\ntest = values[split_index:, :]\n# split into input and outputs\ntrain_X, train_y = train[:, :-1], train[:, -1]\ntest_X, test_y = test[:, :-1], test[:, -1]\n# reshape input to be 3D [samples, timesteps, features]\ntrain_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\ntest_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\nprint(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n \n# design network\nmodel = Sequential()\nmodel.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\nmodel.add(Dense(1))\nmodel.compile(loss='mae', optimizer='adam')\n# fit network\nhistory = model.fit(train_X, train_y, epochs=10, batch_size=72, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n# plot history\npyplot.plot(history.history['loss'], label='train')\npyplot.plot(history.history['val_loss'], label='test')\npyplot.legend()\npyplot.show()\n \n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-18T05:08:45.412230Z","iopub.execute_input":"2024-04-18T05:08:45.412526Z","iopub.status.idle":"2024-04-18T05:08:59.652916Z","shell.execute_reply.started":"2024-04-18T05:08:45.412501Z","shell.execute_reply":"2024-04-18T05:08:59.651974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_X[:, :].shape\n","metadata":{"execution":{"iopub.status.busy":"2024-04-18T05:08:59.654151Z","iopub.execute_input":"2024-04-18T05:08:59.654515Z","iopub.status.idle":"2024-04-18T05:08:59.660805Z","shell.execute_reply.started":"2024-04-18T05:08:59.654482Z","shell.execute_reply":"2024-04-18T05:08:59.659997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yhat = model.predict(test_X)\ntest_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-18T05:08:59.662039Z","iopub.execute_input":"2024-04-18T05:08:59.662406Z","iopub.status.idle":"2024-04-18T05:09:00.953213Z","shell.execute_reply.started":"2024-04-18T05:08:59.662373Z","shell.execute_reply":"2024-04-18T05:09:00.952440Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_X.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-18T05:09:00.954271Z","iopub.execute_input":"2024-04-18T05:09:00.954574Z","iopub.status.idle":"2024-04-18T05:09:00.960077Z","shell.execute_reply.started":"2024-04-18T05:09:00.954548Z","shell.execute_reply":"2024-04-18T05:09:00.959171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yhat.shape\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-18T05:09:00.961146Z","iopub.execute_input":"2024-04-18T05:09:00.961412Z","iopub.status.idle":"2024-04-18T05:09:00.971167Z","shell.execute_reply.started":"2024-04-18T05:09:00.961390Z","shell.execute_reply":"2024-04-18T05:09:00.970364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# invert scaling for forecast\n# For t-2, t-1 timestamps\ninv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)\ninv_yhat =inv_yhat.reshape(-1,8)\ninv_yhat = scaler.inverse_transform(inv_yhat)\ninv_yhat = inv_yhat[:,0]\n","metadata":{"execution":{"iopub.status.busy":"2024-04-18T05:32:46.265510Z","iopub.execute_input":"2024-04-18T05:32:46.266216Z","iopub.status.idle":"2024-04-18T05:32:46.274998Z","shell.execute_reply.started":"2024-04-18T05:32:46.266185Z","shell.execute_reply":"2024-04-18T05:32:46.274108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# invert scaling for actual\ntest_y = test_y.reshape((len(test_y), 1))\ninv_y = concatenate((test_y, test_X[:, 1:]), axis=1)\ninv_y = inv_y.reshape(-1,8)\ninv_y = scaler.inverse_transform(inv_y)\ninv_y = inv_y[:,0]\n# calculate RMSE\nrmse = sqrt(mean_squared_error(inv_y, inv_yhat))\nprint('Test RMSE: %.3f' % rmse)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-18T05:32:48.867793Z","iopub.execute_input":"2024-04-18T05:32:48.868584Z","iopub.status.idle":"2024-04-18T05:32:48.879516Z","shell.execute_reply.started":"2024-04-18T05:32:48.868552Z","shell.execute_reply":"2024-04-18T05:32:48.878625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yhat1 = model.predict(train_X)\ntrain_X = train_X.reshape((train_X.shape[0], train_X.shape[2]))\n                          # invert scaling for forecast train\nfrom numpy import concatenate\ninv_yhat1 = concatenate((yhat1, train_X[:, 1:]), axis=1)\n# inv_yhat1 = inv_yhat1.reshape(-1,8)\n# inv_yhat1 = scaler.inverse_transform(inv_yhat1)\ninv_yhat1 = inv_yhat1[:,0]\n                          # invert scaling for actual train\ntrain_y = train_y.reshape((len(train_y), 1))\ninv_y1 = concatenate((train_y, train_X[:, 1:]), axis=1)\n# inv_y1 = inv_y1.reshape(-1,8)\n# inv_y1 = scaler.inverse_transform(inv_y1)\ninv_y1 = inv_y1[:,0]\n                          \nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\n# Assuming y_true contains the actual target values and y_pred contains the predicted values\nmae = mean_absolute_error(inv_y1, inv_yhat1)\nmse = mean_squared_error(inv_y1, inv_yhat1)\nrmse = mean_squared_error(inv_y1, inv_yhat1, squared=False)\n\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\naccurcay1 = r2_score(inv_y1, inv_yhat1)\naccurcay1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}